#libraries needed
library(gplots)
library(dplyr)
library(purrr)
library(gridExtra)
library(reshape2)
library(cowplot)
library(ggplot2)
library(RColorBrewer)
library(grid) 
library(tidyr)
library(data.table)
library(progress)


'%ino%' <- function(x, table) {
    xSeq <- seq(along = x)
    names(xSeq) <- x
    Out <- xSeq[as.character(table)]
    Out[!is.na(Out)]
}


average_columns_pairs <- function(df) {
  n <- ncol(df)
  if(n %% 2 != 0) {
    stop("Number of columns must be even")
  }

  averaged_cols <- vector("list", length = n/2)
  
  for(i in seq(1, n, by = 2)) {
    averaged_cols[[i %/% 2 + 1]] <- rowMeans(df[,c(i, i+1)])
  }

  new_df <- data.frame(averaged_cols)
  
  # Rename columns for clarity
  colnames(new_df) <- paste0("Avg_", seq(1, n/2))
  
  return(new_df)
}

replace_rownames <- function(df_main, df_mapping, old_name_col, new_name_col) {
  # Create a lookup table
  lookup <- setNames(df_mapping[[new_name_col]], df_mapping[[old_name_col]])
  
  # Replace row names
  rownames(df_main) <- lookup[rownames(df_main)]
  
  return(df_main)
}


max_value_rows <- function(df, factor_col, value_col) {
  
  # First, convert column names to symbols for tidy evaluation
  factor_col <- ensym(factor_col)
  value_col <- ensym(value_col)
  
  df %>%
    # group by factor
    group_by(!!factor_col) %>%
    # find the row with the max value
    filter(!!value_col == max(!!value_col, na.rm = TRUE)) %>%
    # select the first row in case of ties
    slice(1) %>%
    # ungroup the dataframe
    ungroup()
}

adaptive_rolling_average_across_columns <- function(df, num_output_columns, overlap) {
  num_columns <- ncol(df)
  total_size <- num_columns + overlap * (num_output_columns - 1)
  base_window_size <- ceiling(total_size / num_output_columns)
  
  extra = base_window_size * num_output_columns - total_size
  window_sizes = rep(base_window_size, num_output_columns)
  
  if (extra > 0) {
    for (i in (num_output_columns - extra + 1):num_output_columns) {
      window_sizes[i] <- window_sizes[i] - 1
    }
  }
  
  df_avg <- data.frame(matrix(nrow = nrow(df), ncol = num_output_columns))
  start_col <- 1
  
  for (i in 1:num_output_columns) {
    end_col <- min(start_col + window_sizes[i] - 1, num_columns)  # Avoid exceeding the number of columns
    window <- df[, start_col:end_col, drop = FALSE]
    df_avg[, i] <- rowMeans(window, na.rm = TRUE)
    colnames(df_avg)[i] <- paste0("Avg_", start_col, "-", end_col)
    start_col <- end_col + 1 - overlap
  }
  
  row.names(df_avg) <- row.names(df)
  
  return(df_avg)
}


animal_analysis <- function(animal_name,ntimes,num_patterns) {
  

  exprs_file <- paste0("../../ANIMAL_DATA/EXPRMATS/", animal_name, "_EXPRS.txt")
  order_file <- paste0("../../ANIMAL_DATA/EXPRMATS/", animal_name, "_info.txt")
  OGs_file <- paste0("../../ANIMAL_DATA/orthofinder/orthogroupDF/animalOGs/", animal_name, "_TRX_OGdf.txt")
  
  animal_exprs <- read.table(exprs_file, header=T, row.names=1,sep="\t")
  animal_order <- read.table(order_file, header=T,sep="\t")
  animal_OGs <- read.table(OGs_file, header=T,sep="\t")

  if(ntimes=="all"){ntimes=length(animal_exprs[1,])}
  if(ntimes=="half"){ntimes=round(length(animal_exprs[1,])/2)}
 
  animal_OGs[,3] <- as.character(sapply(animal_OGs[,2], function(x) {strsplit(x, "\\.p", fixed = FALSE)[[1]][1]}))
  
  
  animal_exprs<-animal_exprs[,animal_order[1:ntimes,1]]
  #convert animal_exprs data to TPMs as in 2012 paper
  animal_colsums<-colSums(animal_exprs)
  animal_exprs <- sweep(animal_exprs, 2, animal_colsums, "/")
  animal_exprs<-animal_exprs*10^6
  
  ###this could remove all rows with zero variance (where all values are the same). Not sure if it is good to do here.
  #keep <- apply(animal_exprs, 1, function(x) #length(unique(x[!is.na(x)])) != 1)
  #animal_exprs<-animal_exprs[keep, ]
  ####
  
  #y_animal <- DGEList(animal_exprs)
  #keep <- filterByExpr(y_animal)
  #y_animal <- y_animal[keep, ]
  #y_animal <- estimateDisp(y_animal)
  #plotMDS(y_animal)
  
  #cpms_animal <- edgeR::cpm(y_animal, offset = y$offset, log = FALSE)
  #logcounts_animal <- edgeR::cpm(y_animal, offset = y$offset, log = TRUE)
  mintpm<-min(animal_exprs[animal_exprs>0])/2
  logcounts_animal<-log(animal_exprs+mintpm, base=2)

  ###make sure data frames are in time-order and reduced:
  #cpms_animal<-cpms_animal[,animal_order[1:ntimes,1]]
  logcounts_animal<-logcounts_animal[,animal_order[1:ntimes,1]]
  
  
  #get all orthologs of timeDEs from Cperk--actually doesn't restrict to timeDEs unless Cperk_OGs2 is restricted to timeDEs (which it is currentlt not--01-31-2024
  animal_OGs_timeDEs <- animal_OGs[animal_OGs[,1] %in% Cperk_OGs2[,1], ]
  
  animalCons <- unique(animal_OGs_timeDEs[,3])
  
  #get expressed genes (some might not be even if they have an ortholog!)
  animalConsPres <- rownames(logcounts_animal)[rownames(logcounts_animal) %in% animalCons]

  mygenes <- animalConsPres
  #get variance per row, but only for columns within desired timeframe (e.g. ntimes).
  myvars <- rowVars(as.matrix(2^logcounts_animal[mygenes,]))
  names(myvars) <- mygenes 
  animal_OGs2 <- animal_OGs[animal_OGs[,3] %in% mygenes, ]
  #because had to remove info from gene, and some genes had multiple proteins, end up with duplicate rows per gene. Remove here without consequence for expression analyses (I think).
  animal_OGs2 <- animal_OGs2[!duplicated(animal_OGs2$V3), ]
  animal_OGs2$vars <- myvars
  colnames(animal_OGs2)[3]<-"TRX_name"
  
  ###we are going to try to use all OGs rather than just the OGs with the max variance... will see!
  #animal_OGs3 <- as.data.frame(max_value_rows(animal_OGs2,"Orthogroup","vars"))
  #animal_OGplot <- as.data.frame(logcounts_animal[animal_OGs3[,3],])
  animal_OGplot <- as.data.frame(logcounts_animal[mygenes,])
  
  #animal_OGplot <- replace_rownames(animal_OGplot, animal_OGs3, "V3", "Orthogroup")

  animal_OGplot_red <- adaptive_rolling_average_across_columns(animal_OGplot[,1:ntimes], num_patterns,1)
  plotMDS(animal_OGplot_red)
  
  animal_OGplot_scale <- t(scale(t(animal_OGplot_red)))
  
  #rescale data to exist within -2,2 -- data is already pretty close to this, it just squeezes/expands all data to be on the same scale for correlation mapping and plotting. 
  animal_OGplot_scale <-rescale(animal_OGplot_scale, to = c(-2, 2))
  
  ###add column of OG names to data frame
  animal_OGplot_scale <- as.data.frame(animal_OGplot_scale) %>% rownames_to_column("TRX_name")
  animal_OGplot_scale <- animal_OGplot_scale %>% left_join(animal_OGs2, by = "TRX_name")
  
  ###rename rows to gene name, remove unneeded columns:
  rownames(animal_OGplot_scale)<-animal_OGplot_scale$TRX_name
  genecol<-paste(animal_name, "_TRX", sep = "")
  animal_OGplot_scale<-animal_OGplot_scale[, !(colnames(animal_OGplot_scale) %in% c("TRX_name", genecol, "vars"))]
  
  return(animal_OGplot_scale)
}

rename_columns <- function(list_of_matrices) {
  # Get the column names from the first matrix
  column_names <- colnames(list_of_matrices[[1]])

  # Rename columns of all matrices in the list
  for (i in 2:length(list_of_matrices)) {
    colnames(list_of_matrices[[i]]) <- column_names
  }

  return(list_of_matrices)
}



rowwise_distance <- function(matrix1, matrix2) {
  # Find the common row names in both matrices
  common_rows <- intersect(rownames(matrix1), rownames(matrix2))

  # Subset the matrices to only include the common rows
  matrix1_common <- matrix1[common_rows, ]
  matrix2_common <- matrix2[common_rows, ]

  # Initialize an empty vector to store the distances
  distances <- numeric(length(common_rows))

  # For each row in the common rows, compute the Euclidean distance
  for (i in seq_len(length(common_rows))) {
    distances[i] <- dist(rbind(matrix1_common[i, ], matrix2_common[i, ]))
  }

  # Set the names of the distances to be the row names
  names(distances) <- common_rows

  # Return the distances
  return(distances)
}

rowwise_min_avg_distance <- function(matrices_list,which_average) {
  # Check that the list is not empty
  if(length(matrices_list[which_average]) < 2) {
    stop("Need at least two matrices for comparison")
  }
   
  # Extract the "Orthogroup" column from each dataframe into a list of vectors
  orthogroup_list <- lapply(matrices_list[which_average], function(df) df$Orthogroup)
  
  # Find the common values
  common_orthogroups <- Reduce(intersect, orthogroup_list)
  
  matrices_common <- lapply(matrices_list, function(df) df[df$Orthogroup %in% common_orthogroups, ])
  #################################################################
  
  
  # Step 1: Create a new list of matrices to store the results
  matrices_filtered <- vector("list", length(matrices_list))
  names(matrices_filtered) <- names(matrices_list)
  
  # Vector to store average distances for each orthogroup
  avg_distances <- rep(0, length(common_orthogroups))
  names(avg_distances) <- common_orthogroups
  
  # Steps 2 to 7: Iterate over the orthogroups
  for (i in seq_len(length(common_orthogroups))) {
    orthogroup <- common_orthogroups[i]
    ref_rows <- matrices_list[[1]][matrices_list[[1]]$Orthogroup == orthogroup, ]
  
    # Array to store minimum distances for each matrix
    min_distances <- rep(Inf, length(matrices_list))
    
    # Iterate over the matrices in the list
    for (j in seq_len(length(matrices_list))) {
      # Get the rows corresponding to the current orthogroup
      rows <- matrices_list[[j]][matrices_list[[j]]$Orthogroup == orthogroup, ]
      # Iterate over the reference rows
      for (k in seq_len(nrow(ref_rows))) {
        min_distance_k <- Inf
        min_row_k <- NULL
        # Iterate over the rows in the current matrix
        for (l in seq_len(nrow(rows))) {
          # Compute the distance between the current row and the k-th reference row
          non_na_idx <- which(!is.na(rows[l, -1]) & !is.na(ref_rows[k, -1]))
          if (length(non_na_idx) == 0) next # Skip to the next iteration if all values are NA
          distance <- dist(rbind(rows[l, non_na_idx], ref_rows[k, non_na_idx]), method="manhattan")
          if (is.na(distance)) next # Skip to the next iteration if distance is NA
          # Update the minimum distance and row for the k-th reference row
          if (distance < min_distance_k) {
            min_distance_k <- distance
            min_row_k <- rows[l, ]
          }
        }
        # Update the minimum distance for the current matrix
        if (min_distance_k < min_distances[j]) {
          min_distances[j] <- min_distance_k
        }
      }
      
      # Step 8: Add the row that has the minimum average distance to the new matrix
      if (is.null(matrices_filtered[[j]])) {
        matrices_filtered[[j]] <- min_row_k
      } else {
        matrices_filtered[[j]] <- rbind(matrices_filtered[[j]], min_row_k)
      }
    }
    
    # Update the average distance for the current orthogroup
    avg_distances[i] <- mean(min_distances[which_average])
  }
  
  # Change the row names of the filtered matrices to be the orthogroup names
  for (i in seq_len(length(matrices_filtered))) {
    rownames(matrices_filtered[[i]]) <- matrices_filtered[[i]]$Orthogroup
  }
  
  matrices_filtered <- lapply(matrices_filtered, function(mat) {subset(mat, select = -which(colnames(mat) == "Orthogroup"))})
  
  # Print the orthogroups in order of increasing average distance
  avg_distances_ordered<-names(avg_distances)[order(avg_distances)]
  
  result <- list(distances = avg_distances_ordered, matrices = matrices_filtered)
    
  
  # Return the ordered rows and their corresponding average distance matrices
  return(result)
}


rowwise_max_cos_similarity <- function(matrices_list) {
  # Check that the list is not empty
  if(length(matrices_list) < 2) {
    stop("Need at least two matrices for comparison")
  }
  
  # Find the common row names in all matrices
  common_rows <- Reduce(intersect, lapply(matrices_list, rownames))

  # Subset the matrices to only include the common rows
  matrices_common <- lapply(matrices_list, function(mat) mat[common_rows, ])

  # Initialize a vector to store the cosine similarities
  cos_similarities <- numeric(length(common_rows))
  names(cos_similarities) <- common_rows

  # Function to calculate cosine similarity
  cosine_similarity <- function(x, y) {
    sum(x * y) / (sqrt(sum(x^2)) * sqrt(sum(y^2)))
  }

  # Compute max cosine similarity for each row across all matrices
  for (i in seq_len(length(common_rows))) {
    similarities <- numeric(length(matrices_list))
    for (j in seq_len(length(matrices_list))){
      similarities[j] <- cosine_similarity(matrices_common[[j]][i, ], matrices_common[[1]][i, ]) # Using first matrix as reference
    }
    cos_similarities[i] <- max(similarities)
  }

  # Order the cosine similarities
  cos_similarities_ordered <- cos_similarities[order(-cos_similarities)]

  # Return the ordered rows and their corresponding max cosine similarities
  return(cos_similarities_ordered)
}

select_OGmats<-function(matrices_list,which_average,simtype) {
 # Compute minimum average distances
  if (simtype == "cos") {
    avg_distances_ordered <- rowwise_max_cos_similarity(matrices_list,which_average)
  } else if (simtype == "euc") {
    avg_distances_ordered <- rowwise_min_avg_distance(matrices_list,which_average)
  }
}

plot_heatmaps <- function(avg_distances_ordered, which_plot, n, simtype, uorder, plotyn, ncols = 3) {
 
  # Get top n rows
  top_n_rows <- avg_distances_ordered$distances[1:n]


  # Subset matrices in matrices_list to top n rows
  subset_matrices1 <- lapply(avg_distances_ordered$matrices, function(mat) mat[top_n_rows, ])

  # Subset matrices for plotting to top n rows, handling missing rownames
  subset_matrices2 <- lapply(avg_distances_ordered$matrices[which_plot], function(mat) {
    common_rows <- intersect(top_n_rows, rownames(mat))
    mat[common_rows, , drop = FALSE]
  })

  ####code for making a single legend for plotting later.
  n_colors <- 100
  colors <- rev(brewer.pal(11, "Spectral"))
  color_palette <- colorRampPalette(colors)(n_colors)
  dummy_data <- expand.grid(x = 1:10, y = 1:10)
  dummy_data$value <- runif(100)
  global_min <- min(unlist(lapply(subset_matrices2, function(x) min(x, na.rm = TRUE))))
  global_max <- max(unlist(lapply(subset_matrices2, function(x) max(x, na.rm = TRUE))))
  dummy_heatmap <- ggplot(data = dummy_data, aes(x = x, y = y, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = color_palette, limits = c(global_min, global_max)) +
  theme_void() +
  theme(legend.position = "bottom", 
        legend.key.height = unit(0.25, "cm"), 
        legend.text = element_text(size = 5)) +
  labs(fill = NULL)  # Remove the legend label

  # Extract the legend
  dummy_legend <- cowplot::get_legend(dummy_heatmap)
  ############################################
  
  # Calculate pairwise distances between subset matrices
  distance_matrix <- pairwise_distance_heatmap(subset_matrices2, top_n_rows, FALSE)

  ordered_indices <- order(distance_matrix[1,])

  # Order the subset matrices based on the clustering
  ordered_matrices <- subset_matrices2[ordered_indices]

  # Create a list to store the plots
  plot_list <- vector("list", length(ordered_indices))

  # Loop over ordered matrices and generate heatmaps
  for (i in seq_along(ordered_matrices)) {
    if(i == 1) {
		# Calculate hierarchical clustering of the first matrix
		hc <- hclust(dist(subset_matrices1[[1]]))

		# Get order of rows from hierarchical clustering
		hc_indices <- order.dendrogram(as.dendrogram(hc))

		# Convert indices to row names
		hc_order <- rownames(subset_matrices1[[1]])[hc_indices]
    }
    sub_matrix <- subset_and_order(ordered_matrices[[i]], hc_order)
    if (uorder == 1) {
      # Generate a heatmap using heatmap.2
      plot_list[[i]] <- heatmap.2(as.matrix(sub_matrix),
                                   col = rev(morecols(50)),
                                   trace = "none",
                                   scale = "row",
                                   margins = c(9, 9),
                                   Colv = FALSE,
                                   Rowv = TRUE,
                                   main = names(subset_matrices2)[ordered_indices[i]])
    } else {
      # Create a data frame with rownames and colnames for plotting
	  df <- as.data.frame(sub_matrix)
	  df$rownames <- rownames(sub_matrix)  # Convert rownames to a column
	  df <- df %>% pivot_longer(cols = -rownames, names_to = "colnames", values_to = "value")     

      # Generate a heatmap plot using ggplot2
      n_colors <- 100
      colors <- rev(brewer.pal(11, "Spectral"))
      color_palette <- colorRampPalette(colors)(n_colors)
      p <- ggplot(data = df, aes(x = colnames, y = factor(rownames, levels = hc_order), fill = value)) +
  geom_tile(width = 0.9, height = 0.9) +
  scale_fill_gradientn(colors = color_palette, limits = c(global_min, global_max)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
        axis.text.y = element_text(size = 3),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "none",
		plot.title = element_text(hjust = 0.5, size=8)) +
  labs(title = names(subset_matrices2)[ordered_indices[i]])
      # Add the plot object to the list
      plot_list[[i]] <- p
    }
  }

  # Combine all the plots in the list
  combined_plot <- patchwork::wrap_plots(plotlist = plot_list, ncol=ncols)

  # Print the combined plot
  if (plotyn == 1) {
    final_plot <- cowplot::plot_grid(dummy_legend, combined_plot, ncol = 1, rel_heights = c(0.1, 0.9))
    print(final_plot)
  }

  # Return the combined plot
  return(hc_order)
}

subset_and_order <- function(mat, order) {
  missing_rows <- setdiff(order, rownames(mat))
  if(length(missing_rows) > 0) {
    mat <- rbind(mat, matrix(NA, nrow = length(missing_rows), ncol = ncol(mat),
                             dimnames = list(missing_rows, colnames(mat))))
  }
  mat <- mat[order, ]
  return(mat)
}

pairwise_distance_heatmap <- function(list_of_matrices, vect_of_rownames,plotyn) {
  
  # Get the names of matrices
  matrix_names <- names(list_of_matrices)
  
 # Subset each matrix based on provided rownames and replace NA rows with zeros
  subset_matrices <- lapply(list_of_matrices, function(mat) {
    mat_subset <- mat[rownames(mat) %in% vect_of_rownames, ]
    # Replace NA rows with zeros
    is_na_row <- apply(mat_subset, 1, function(row) all(is.na(row)))
    mat_subset[is_na_row, ] <- 0
    return(mat_subset)
  })
  
  # Find common row names across all matrices
  common_rows <- Reduce(intersect, lapply(subset_matrices, rownames))
  
  # Calculate average pairwise distances between common rows
  distance_matrix <- matrix(0, nrow = length(list_of_matrices), ncol = length(list_of_matrices),
                            dimnames = list(matrix_names, matrix_names))
  
  for (i in seq_along(subset_matrices)) {
    for (j in seq_along(subset_matrices)) {
      matrix1 <- subset_matrices[[i]]
      matrix2 <- subset_matrices[[j]]
      
      # Subset matrices based on common rows
      matrix1_common <- matrix1[common_rows, , drop = FALSE]
      matrix2_common <- matrix2[common_rows, , drop = FALSE]
      
      # Calculate pairwise distances between common rows
      row_distances <- rowSums((matrix1_common - matrix2_common)^2)
      
      # Compute average distance
      average_distance <- mean(row_distances, na.rm = TRUE)
      
      distance_matrix[i, j] <- average_distance
    }
  }
  
  # Generate color palette
  Colors=rev(brewer.pal(11,"Spectral"))
  
  if(plotyn==1){
  # Plot the distance matrix using heatmap.2
  heatmap.2(distance_matrix, col = Colors,
            main = "Pairwise Distance Heatmap",
            trace = "none",scale="none")  # Disable trace lines around cells
  }
	return(distance_matrix)
}

orthogroup_avg <- function(matrices_list, which_average = seq_along(matrices_list)) {
  # Check that the list is not empty
  if(length(matrices_list) < 1) {
    stop("Need at least one matrix")
  }

  # Convert data.frames to data.tables for efficient operations
  matrices_list <- lapply(matrices_list, as.data.table)

  # Find the common orthogroups
  common_orthogroups <- Reduce(intersect, lapply(matrices_list, function(dt) dt$Orthogroup))

  # Calculate averages for each orthogroup in each matrix, keeping only numeric columns
  matrices_avg <- lapply(matrices_list, function(dt) {
    dt_common <- dt[Orthogroup %in% common_orthogroups, ]
    dt_common[, lapply(.SD, mean, na.rm = TRUE), by = Orthogroup, .SDcols = sapply(dt, is.numeric)]
  })

  # Initialize list for distances
  avg_distances <- vector("list", length(common_orthogroups))
  names(avg_distances) <- common_orthogroups

  for (orthogroup in common_orthogroups) {
    dist_list <- vector("numeric", length(which_average))
    for (i in which_average) {
      if (orthogroup %in% matrices_avg[[i]]$Orthogroup) {
        numeric_cols <- setdiff(names(matrices_avg[[i]]), "Orthogroup")
        dist_list[i] <- mean(abs(unlist(matrices_avg[[1]][Orthogroup == orthogroup, numeric_cols, with = FALSE]) -
                                unlist(matrices_avg[[i]][Orthogroup == orthogroup, numeric_cols, with = FALSE])), na.rm = TRUE)
      } else {
        dist_list[i] <- NA
      }
    }
    avg_distances[[orthogroup]] <- mean(dist_list, na.rm = TRUE)
  }

  # Order the orthogroups based on their average distance to the first matrix
  orthogroups_ordered <- names(avg_distances)[order(unlist(avg_distances), na.last = NA)]

  # Reorder the rows in each matrix according to the ordered orthogroups
  matrices_ordered <- lapply(matrices_avg, function(dt) {
    dt_ordered <- dt[Orthogroup %in% orthogroups_ordered, ][order(match(dt$Orthogroup, orthogroups_ordered)), ]
    setDF(dt_ordered)  # convert back to data frame
    row.names(dt_ordered) <- dt_ordered$Orthogroup  # set orthogroups as row names
    dt_ordered$Orthogroup <- NULL  # remove Orthogroup column
    dt_ordered  # return the data frame
  })

  # Remove rows where all numeric columns are NA
  matrices_ordered <- lapply(matrices_ordered, function(df) {
    df[!apply(is.na(df), 1, all), ]
  })

  result <- list(distances = orthogroups_ordered, matrices = matrices_ordered)
  return(result)
}

min_distance_rows <- function(matrices_list, matrices_indices = seq_len(length(matrices_list))) {
  # Check that the list and indices are not empty
  if(length(matrices_list) < 1) {
    stop("Need at least one matrix")
  }
  if(length(matrices_indices) < 1) {
    stop("Need at least one matrix index")
  }

  # Convert data.frames to data.tables for efficient operations
  matrices_list <- lapply(matrices_list, function(df) {
    dt <- setDT(df)
    dt[, OG_number := .I]
    dt[, OG_count := seq_len(.N), by = Orthogroup]
    dt[, Orthogroup := paste0(Orthogroup, ".", OG_count)]
    dt
  })

  first_matrix <- matrices_list[[1]]
  matrices_rest <- matrices_list[matrices_indices]

  numeric_cols <- setdiff(names(first_matrix), c("Orthogroup", "OG_number", "OG_count"))

  # Initialize progress bar
  pb <- progress_bar$new(total = nrow(first_matrix), format = "Calculating distances: [:bar] :percent")

  # Initialize list for distances
  distances_list <- vector("list", nrow(first_matrix))

  # Loop over the rows of the first matrix
  for (i in seq_len(nrow(first_matrix))) {
    pb$tick()  # Update progress bar
    orthogroup <- first_matrix[i, Orthogroup]
    first_row <- unlist(first_matrix[i, numeric_cols, with = FALSE])
    for (j in seq_along(matrices_rest)) {
      dist_min <- Inf
      row_min <- NULL
      for (k in seq_len(nrow(matrices_rest[[j]]))) {
        rest_row <- unlist(matrices_rest[[j]][k, numeric_cols, with = FALSE])
        dist <- sqrt(sum((first_row - rest_row)^2, na.rm = TRUE))
        if (dist < dist_min) {
          dist_min <- dist
          row_min <- matrices_rest[[j]][k, ]
        }
      }
      matrices_rest[[j]][row_min$OG_number, ] <- row_min
    }
    distances_list[[i]] <- orthogroup
  }

  # Convert data.tables back to data.frames
  matrices_list <- lapply(c(list(first_matrix), matrices_rest), as.data.frame)

  # Set orthogroups as row names
  matrices_list <- lapply(matrices_list, function(df) {
    row.names(df) <- df$Orthogroup
    df$Orthogroup <- NULL
    df$OG_number <- NULL
    df$OG_count <- NULL
    df
  })

  result <- list(distances = distances_list, matrices = matrices_list)
  return(result)
}



 
rowwise_min_avg_distance_ORTHOLOGUES <- function(matrices_list, which_average, orthologue_files) {
  # Check that the list is not empty
  if(length(matrices_list[which_average]) < 2) {
    stop("Need at least two matrices for comparison")
  }
   
  # Extract the "Orthogroup" column from each dataframe into a list of vectors
  orthogroup_list <- lapply(matrices_list[which_average], function(df) df$Orthogroup)
  
  # Find the common values
  common_orthogroups <- Reduce(intersect, orthogroup_list)
  
  matrices_common <- lapply(matrices_list, function(df) df[df$Orthogroup %in% common_orthogroups, ])
  
  # Define the common directory for orthologue_files
  common_dir <- "../Cperk_orthologues/"

  # Read orthologue data from files in the specified directory
  orthologue_data <- lapply(orthologue_files, function(file) {
  file_path <- paste0(common_dir, file)  # Construct the complete path to the file
  read.table(file_path, header = TRUE, sep = "\t", quote = "", stringsAsFactors = FALSE, col.names = c("Orthogroup", "Gene", "Orthologue"))
  })

  # Initialize lists for new matrices and minimum distances
  matrices_filtered <- vector("list", length(matrices_list))
  names(matrices_filtered) <- names(matrices_list)
  
  min_distances <- rep(Inf, length(matrices_list))
  names(min_distances) <- names(matrices_list)
  
  # Iterate over the orthogroups
  for (i in seq_len(length(common_orthogroups))) {
    orthogroup <- common_orthogroups[i]
    print(orthogroup)
    # Filter orthologue data for current orthogroup
    orthogroup_data <- lapply(orthologue_data, function(df) df[df$Orthogroup == orthogroup, ])
    
    ref_df <- matrices_list[[1]]
    ref_rows <- ref_df[ref_df$Orthogroup == orthogroup, ]
    
    # add the reference rows to the first matrix in matrices_filtered
    matrices_filtered[[1]] <- rbind(matrices_filtered[[1]], ref_rows)
    
    # Iterate over the matrices in the list starting from the second one
    for (j in 2:length(matrices_list)) {
      df <- matrices_list[[j]]
      
      orthologue_rows <- vector("list", nrow(ref_rows))
      names(orthologue_rows) <- rownames(ref_rows)
      
      # Iterate over the genes for the current orthogroup
	  min_distance <- Inf
	  min_row <- NULL
      for (k in seq_len(nrow(ref_rows))) {
        gene <- rownames(ref_rows)[k]
		print(gene)
        orthologues <- unlist(strsplit(orthogroup_data[[j-1]]$Orthologue[grep(gene,orthogroup_data[[j-1]]$Gene)], ", "))
		print(orthologues)
		if(length(orthologues)==0){orthologue_rows$gene<-NULL}
        orthologues_clean <- gsub("\\.p.*", "", orthologues)
        # Find the orthologue that minimizes the distance
        for (orthologue in orthologues_clean) {
          row <- df[orthologue, ]
          if (nrow(row) > 0) {
            non_na_idx <- which(!is.na(row[1, -1]) & !is.na(ref_rows[k, -1]))
            if (length(non_na_idx) > 0) {
              distance <- dist(rbind(row[1, non_na_idx], ref_rows[k, non_na_idx]), method = "manhattan")
			  print(c(orthologue,distance))
              if (!is.na(distance) && distance < min_distance) {
                min_distance <- distance
                min_row <- row
              }
            }
          }
        }
        
        # Store the orthologue row that minimizes the distance
        if(length(orthologues)>0){
			orthologue_rows[[gene]]$orow <- min_row
			orthologue_rows[[gene]]$Distance <- min_distance
		}
      }
      
      # Compute the minimum average distance
      min_distances[j] <- mean(unlist(lapply(orthologue_rows, function(row) if (nrow(row) > 0) min(row$Distance, na.rm = TRUE) else Inf)))
      
      # Add the orthologue rows to the new matrix
      matrices_filtered[[j]] <- do.call(rbind, orthologue_rows)
    }
  }
  
  # Change the row names of the filtered matrices to be the orthogroup names
  for (i in seq_len(length(matrices_filtered))) {
    rownames(matrices_filtered[[i]]) <- matrices_filtered[[i]]$Orthogroup
  }
  
  matrices_filtered <- lapply(matrices_filtered, function(mat) {subset(mat, select = -which(colnames(mat) == "Orthogroup"))})
  
  # Print the orthogroups in order of increasing average distance
  avg_distances_ordered <- names(min_distances)[order(min_distances)]
  
  result <- list(distances = avg_distances_ordered, matrices = matrices_filtered)
    
  # Return the ordered rows and their corresponding average distances
  return(result)
}


#######################################################
#########get the index of a match from within a list, even if each element of the list has more than one sub-element.
match_indices <- function(gene, genes) {
  indices <- which(sapply(genes, function(x) any(x == gene)))
  return(indices)
}


####################################################



animal_analysis_SHUFF <- function(animal_name,ntimes,num_patterns) {
  

  exprs_file <- paste0("../../ANIMAL_DATA/EXPRMATS/", animal_name, "_EXPRS.txt")
  order_file <- paste0("../../ANIMAL_DATA/EXPRMATS/", animal_name, "_info.txt")
  OGs_file <- paste0("../../ANIMAL_DATA/orthofinder/orthogroupDF/animalOGs/", animal_name, "_TRX_OGdf.txt")
  
  animal_exprs <- read.table(exprs_file, header=T, row.names=1,sep="\t")
  animal_order <- read.table(order_file, header=T,sep="\t")
  animal_OGs <- read.table(OGs_file, header=T,sep="\t")

  if(ntimes=="all"){ntimes=length(animal_exprs[1,])}
  if(ntimes=="half"){ntimes=round(length(animal_exprs[1,])/2)}
 
  animal_OGs[,3] <- ifelse(grepl("\\.p", animal_OGs[,2]), 
                          do.call(rbind, strsplit(animal_OGs[,2], ".p", fixed = TRUE))[,1], 
                          animal_OGs[,2])
  
  animal_exprs<-animal_exprs[,animal_order[1:ntimes,1]]
  #convert animal_exprs data to TPMs as in 2012 paper
  animal_colsums<-colSums(animal_exprs)
  animal_exprs <- sweep(animal_exprs, 2, animal_colsums, "/")
  animal_exprs<-animal_exprs*10^6
  
  ###this could remove all rows with zero variance (where all values are the same). Not sure if it is good to do here.
  #keep <- apply(animal_exprs, 1, function(x) #length(unique(x[!is.na(x)])) != 1)
  #animal_exprs<-animal_exprs[keep, ]
  ####
  
  #y_animal <- DGEList(animal_exprs)
  #keep <- filterByExpr(y_animal)
  #y_animal <- y_animal[keep, ]
  #y_animal <- estimateDisp(y_animal)
  #plotMDS(y_animal)
  
  #cpms_animal <- edgeR::cpm(y_animal, offset = y$offset, log = FALSE)
  #logcounts_animal <- edgeR::cpm(y_animal, offset = y$offset, log = TRUE)
  mintpm<-min(animal_exprs[animal_exprs>0])/2
  logcounts_animal<-log(animal_exprs+mintpm, base=2)
  rownames(logcounts_animal) <- sample(rownames(logcounts_animal))

  ###make sure data frames are in time-order and reduced:
  #cpms_animal<-cpms_animal[,animal_order[1:ntimes,1]]
  logcounts_animal<-logcounts_animal[,animal_order[1:ntimes,1]]
  
  
  #get all orthologs of timeDEs from Cperk
  animal_OGs_timeDEs <- animal_OGs[animal_OGs[,1] %in% Cperk_OGs2[,1], ]
  
  animalCons <- unique(animal_OGs_timeDEs[,3])
  
  #get expressed genes (some might not be even if they have an ortholog!)
  animalConsPres <- rownames(logcounts_animal)[rownames(logcounts_animal) %in% animalCons]

  mygenes <- animalConsPres
  #get variance per row, but only for columns within desired timeframe (e.g. ntimes).
  myvars <- rowVars(as.matrix(2^logcounts_animal[mygenes,]))
  names(myvars) <- mygenes 
  animal_OGs2 <- animal_OGs[animal_OGs[,3] %in% mygenes, ]
  #because had to remove info from gene, and some genes had multiple proteins, end up with duplicate rows per gene. Remove here without consequence for expression analyses (I think).
  animal_OGs2 <- animal_OGs2[!duplicated(animal_OGs2$V3), ]
  animal_OGs2$vars <- myvars
  colnames(animal_OGs2)[3]<-"TRX_name"
  
  ###we are going to try to use all OGs rather than just the OGs with the max variance... will see!
  #animal_OGs3 <- as.data.frame(max_value_rows(animal_OGs2,"Orthogroup","vars"))
  #animal_OGplot <- as.data.frame(logcounts_animal[animal_OGs3[,3],])
  animal_OGplot <- as.data.frame(logcounts_animal[mygenes,])
  
  #animal_OGplot <- replace_rownames(animal_OGplot, animal_OGs3, "V3", "Orthogroup")

  animal_OGplot_red <- adaptive_rolling_average_across_columns(animal_OGplot[,1:ntimes], num_patterns,1)
  plotMDS(animal_OGplot_red)
  
  animal_OGplot_scale <- t(scale(t(animal_OGplot_red)))
  
  ###add column of OG names to data frame
  animal_OGplot_scale <- as.data.frame(animal_OGplot_scale) %>% rownames_to_column("TRX_name")
  animal_OGplot_scale <- animal_OGplot_scale %>% left_join(animal_OGs2, by = "TRX_name")
  
  ###rename rows to gene name, remove unneeded columns:
  rownames(animal_OGplot_scale)<-animal_OGplot_scale$TRX_name
  genecol<-paste(animal_name, "_TRX", sep = "")
  animal_OGplot_scale<-animal_OGplot_scale[, !(colnames(animal_OGplot_scale) %in% c("TRX_name", genecol, "vars"))]
  
  return(animal_OGplot_scale)
}

  rename_columns <- function(list_of_matrices) {
  # Get the column names from the first matrix
  column_names <- colnames(list_of_matrices[[1]])

  # Rename columns of all matrices in the list
  for (i in 2:length(list_of_matrices)) {
    colnames(list_of_matrices[[i]]) <- column_names
  }

  return(list_of_matrices)
}


